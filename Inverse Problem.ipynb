{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a45fcf5",
   "metadata": {},
   "source": [
    "### GPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d058dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.current_device()\n",
    "    torch.cuda.device(0)\n",
    "    torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)\n",
    "\n",
    "    # setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f695b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import normal\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import time\n",
    "import cv2\n",
    "from skimage import io\n",
    "import math\n",
    "from scipy.linalg import orth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a1ab9",
   "metadata": {},
   "source": [
    "# Basic Linear Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a784e56",
   "metadata": {},
   "source": [
    "## Condition number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4018660",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = []\n",
    "for e in [1,0.1,0.01,0.001,0.0001]:\n",
    "    A_epsilon = np.array([[1,1],[1,1+e]])\n",
    "    condition = np.linalg.cond(A_epsilon)\n",
    "    cond.append(condition)\n",
    "    print('epsilon: ',e,' conditon number: ',condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f488b6",
   "metadata": {},
   "source": [
    "## Tikhonov Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc2762",
   "metadata": {},
   "source": [
    "### Optimal parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "np.random.seed(1)\n",
    "N = 10000 # number of data size\n",
    "LAMBDA = []\n",
    "X = np.random.randn(2,N)\n",
    "for k in range(5):\n",
    "    MSE = []\n",
    "    epsilon = 10**(-k)\n",
    "    A_epsilon = np.array([[1,1],[1,1+epsilon]]) # (2,2)\n",
    "    eta = np.random.randn(2,N) # (2,N)\n",
    "    Y = np.dot(A_epsilon,X) + 0.01*eta # (2,N)\n",
    "    \n",
    "    for a in range(1,100001):\n",
    "        scale = 1e6\n",
    "        a = a/scale\n",
    "        # Tikhonov solution\n",
    "        X_tik = np.linalg.inv(np.dot(A_epsilon.T,A_epsilon) + a * np.eye(2)).dot(A_epsilon.T).dot(Y)\n",
    "        \n",
    "        # error\n",
    "        error = np.linalg.norm(X_tik-X)**2/X.shape[1]\n",
    "        MSE.append(error)\n",
    "        \n",
    "    Lambda = (MSE.index(min(MSE))+1)/scale    \n",
    "    LAMBDA.append(Lambda)\n",
    "    plt.plot((np.array(range(len(MSE)))+1)/scale,MSE,'-',label='epsilon = '+ str(epsilon))\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$MSE=\\frac{1}{n}\\Vert X_{tik}-X \\Vert$')\n",
    "    plt.xlabel(r'$\\lambda$')\n",
    "    plt.title('Optimal')\n",
    "    #plt.ylim(0)\n",
    "    plt.show()\n",
    "    print('epsilon = ', str(epsilon),' MSE:',min(MSE),' lambda:',Lambda ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099d2655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0006812162305501012, 0.040074347857498874, 0.8146861803659274, 1.03274110047104, 1.0341222194233362]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "np.random.seed(2)\n",
    "N = 2000 # number of data size\n",
    "\n",
    "X = np.random.randn(2,N)\n",
    "MSE = []\n",
    "for k,a in zip(range(5),LAMBDA):\n",
    "    epsilon = 10**(-k)\n",
    "    A_epsilon = np.array([[1,1],[1,1+epsilon]]) \n",
    "    eta = np.random.randn(2,N) \n",
    "    Y = np.dot(A_epsilon,X) + 0.01 * eta \n",
    "\n",
    "    # Tikhonov solution\n",
    "    X_tik = np.linalg.inv(np.dot(A_epsilon.T,A_epsilon) + a * np.eye(2)).dot(A_epsilon.T).dot(Y)\n",
    "        \n",
    "    # error\n",
    "    error = np.linalg.norm(X_tik-X)**2/X.shape[1]\n",
    "    MSE.append(error)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80445331",
   "metadata": {},
   "source": [
    "### L-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curvature\n",
    "def curvature(x,y):\n",
    "    x_t = np.gradient(x)\n",
    "    y_t = np.gradient(y)\n",
    "    xx_t = np.gradient(x_t)\n",
    "    yy_t = np.gradient(y_t)\n",
    "    curvature_val = np.abs(xx_t * y_t - x_t * yy_t) / (x_t * x_t + y_t * y_t)**1.5\n",
    "    curvature_val = curvature_val.tolist()\n",
    "    index = curvature_val.index(max(curvature_val))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "np.random.seed(1)\n",
    "N = 10000 # number of data size\n",
    "LAMBDA = []\n",
    "X = np.random.randn(2,N)\n",
    "for k in range(5):\n",
    "    epsilon = 10**(-k)\n",
    "    A_epsilon = np.array([[1,1],[1,1+epsilon]]) # (2,2)\n",
    "    eta = np.random.randn(2,N) # (2,N)\n",
    "    Y = np.dot(A_epsilon,X) + 0.01*eta # (2,N)\n",
    "    solution = []\n",
    "    residual = []\n",
    "    \n",
    "    for a in range(1,100001):\n",
    "        scale = 1e6\n",
    "        a = a/scale\n",
    "        # Tikhonov solution\n",
    "        X_tik = np.dot(np.dot(np.linalg.inv(np.dot(A_epsilon.T,A_epsilon) + a * np.eye(2)),A_epsilon.T),Y)\n",
    "        \n",
    "        # norm\n",
    "        X_tik_norm = np.linalg.norm(X_tik)\n",
    "        residual_norm = np.linalg.norm(np.dot(A_epsilon,X_tik) - Y)\n",
    "        solution.append(X_tik_norm)\n",
    "        residual.append(residual_norm)\n",
    "        \n",
    "    # L-curve figure\n",
    "    \n",
    "    plt.plot(residual,solution,'.-',label='$\\epsilon$ = '+ str(epsilon))\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$\\log\\Vert X_{Tik} \\Vert$')\n",
    "    plt.xlabel(r'$\\log\\Vert AX_{Tik}-Y^{\\eta} \\Vert$')\n",
    "    plt.title('Discrete L-curve for Tikhonov regularization')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    Lambda = (curvature(residual,solution)+1)/scale\n",
    "    LAMBDA.append(Lambda)\n",
    "    X_tik_Lambda = np.dot(np.dot(np.linalg.inv(np.dot(A_epsilon.T,A_epsilon) + Lambda * np.eye(2)),A_epsilon.T),Y)\n",
    "    MSE = np.linalg.norm(X_tik_Lambda-X)**2 / Y.shape[1]\n",
    "    \n",
    "    print('epsilon: ',epsilon,'lambda: ',Lambda, ' MSE: ',MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "np.random.seed(2)\n",
    "N = 2000 # number of data size\n",
    "\n",
    "X = np.random.randn(2,N)\n",
    "MSE = []\n",
    "for k,a in zip(range(5),LAMBDA):\n",
    "    epsilon = 10**(-k)\n",
    "    A_epsilon = np.array([[1,1],[1,1+epsilon]]) # (2,2)\n",
    "    eta = np.random.randn(2,N) # (2,N)\n",
    "    Y = np.dot(A_epsilon,X) + 0.01*eta # (2,N)\n",
    "\n",
    "    # Tikhonov solution\n",
    "    X_tik = np.linalg.inv(np.dot(A_epsilon.T,A_epsilon) + a * np.eye(2)).dot(A_epsilon.T).dot(Y)\n",
    "        \n",
    "    # error\n",
    "    error = np.linalg.norm(X_tik-X)**2/X.shape[1]\n",
    "    MSE.append(error)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fdd602",
   "metadata": {},
   "source": [
    "### GCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d77de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "np.random.seed(1)\n",
    "N = 10000 # number of data size\n",
    "LAMBDA = []\n",
    "X = np.random.randn(2,N)\n",
    "for k in range(5):\n",
    "    GCV = []\n",
    "    epsilon = 10**(-k)\n",
    "    A_epsilon = np.array([[1,1],[1,1+epsilon]]) # (2,2)\n",
    "    eta = np.random.randn(2,N) # (2,N)\n",
    "    Y = np.dot(A_epsilon,X) + 0.01*eta # (2,N)\n",
    "    \n",
    "    for a in range(1,100001):\n",
    "        scale = 1e6\n",
    "        a = a/scale\n",
    "        # Tikhonov solution\n",
    "        X_tik = np.dot(np.dot(np.linalg.inv(np.dot(A_epsilon.T,A_epsilon) + a * np.eye(2)),A_epsilon.T),Y)\n",
    "        \n",
    "        # GCV\n",
    "        upper = np.linalg.norm(np.dot(A_epsilon,X_tik)-Y)**2\n",
    "        lower = np.trace(np.eye(2) - np.dot(np.dot(A_epsilon,np.linalg.inv(np.dot(A_epsilon.T,A_epsilon)\n",
    "                                                                           + a * np.eye(2))),A_epsilon.T))**2\n",
    "        gcv = upper/lower\n",
    "        GCV.append(gcv)\n",
    "\n",
    "    plt.plot(np.array(range(len(GCV)))/scale,GCV,'-',label='$\\epsilon$ = '+ str(epsilon))\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$GCV(\\lambda)$')\n",
    "    plt.xlabel(r'$\\lambda$')\n",
    "    plt.title('GCV')\n",
    "    plt.ylim(0)\n",
    "    \n",
    "    Lambda = (GCV.index(min(GCV))+1)/scale\n",
    "    LAMBDA.append(Lambda)\n",
    "    X_tik_Lambda = np.dot(np.dot(np.linalg.inv(np.dot(A_epsilon.T,A_epsilon) + Lambda * np.eye(2)),A_epsilon.T),Y)\n",
    "    MSE = np.linalg.norm(X_tik_Lambda-X)**2 / Y.shape[1]\n",
    "    \n",
    "    print('epsilon = ', str(epsilon),' GCV:',min(GCV),' lambda:',Lambda ,' MSE: ',MSE,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65561213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "np.random.seed(2)\n",
    "N = 2000 # number of data size\n",
    "X = np.random.randn(2,N)\n",
    "MSE = []\n",
    "for k,a in zip(range(5),LAMBDA):\n",
    "    epsilon = 10**(-k)\n",
    "    A_epsilon = np.array([[1,1],[1,1+epsilon]]) # (2,2)\n",
    "    eta = np.random.randn(2,N) # (2,N)\n",
    "    Y = np.dot(A_epsilon,X) + 0.01*eta # (2,N)\n",
    "\n",
    "    # Tikhonov solution\n",
    "    X_tik = np.linalg.inv(np.dot(A_epsilon.T,A_epsilon) + a * np.eye(2)).dot(A_epsilon.T).dot(Y)\n",
    "        \n",
    "    # error\n",
    "    error = np.linalg.norm(X_tik-X)**2/X.shape[1]\n",
    "    MSE.append(error)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9fae2",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade4d02",
   "metadata": {},
   "source": [
    "### Forward problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch version (without weight restriction)\n",
    "\n",
    "EPOCH = 500\n",
    "LEARNING_RATE = 0.05\n",
    "N = 10000\n",
    "MSE = []\n",
    "\n",
    "# data generation\n",
    "for k in range(5):\n",
    "    torch.manual_seed(1)\n",
    "    X = torch.randn(2,N)\n",
    "    epsilon = 10**(-k)\n",
    "    A_epsilon = torch.Tensor([[1,1],[1,1+epsilon]]) \n",
    "    eta = torch.randn(2,N)\n",
    "    Y = torch.mm(A_epsilon,X) + 0.01*eta\n",
    "\n",
    "    net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(2,4),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(4,2)\n",
    "    )\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimzer = torch.optim.Adam(net.parameters(),lr = LEARNING_RATE)\n",
    "    #optimzer = torch.optim.SGD(net.parameters(),lr = LEARNING_RATE)\n",
    "\n",
    "# training\n",
    "    for epoch in range(EPOCH):\n",
    "        Y_pred = net(X.T)\n",
    "        loss = criterion(Y_pred,Y.T)\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "        MSE.append(loss.data.numpy())\n",
    "        # test\n",
    "        if epoch == EPOCH-1:\n",
    "            torch.manual_seed(2)\n",
    "            n = 2000\n",
    "            x = torch.randn(2,n)\n",
    "            e = torch.randn(2,n)\n",
    "            y = torch.mm(A_epsilon,x) + 0.01 * e\n",
    "            y_pred = net(x.T)\n",
    "            mse = criterion(y_pred,y.T)\n",
    "            print('epsilon: ',epsilon, 'MSE: ',mse)        \n",
    "    plt.plot(range(EPOCH),MSE[EPOCH*k:EPOCH*(k+1)],'-',label='$\\epsilon$ = '+ str(epsilon))\n",
    "    #plt.ylim(0,3)\n",
    "    #plt.xlim(0)\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$MSE=\\frac{1}{n}\\Vert W_{x}X-Y\\Vert ^{2} $')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.title('Forward Problem (Basic Matrix)')\n",
    "plt.savefig('nn(DPBMnoOutliers).pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04427228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy version (with weight restriction)\n",
    "np.random.seed(1)\n",
    "EPOCH = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "N , D_in , H , D_out = 10000 , 2, 4 , 2 \n",
    "# N is the number of the samples，D_in is the dim of input，\n",
    "# H is the numeber of the notes in hidden layer,D_out is the dim of the output\n",
    "\n",
    "X = np.random.randn(D_in,N) # input(D_in,N)\n",
    "MSE = []\n",
    "for k in range(5):\n",
    "    epsilon = 10**(-k)\n",
    "    A_epsilon = np.array([[1,1],[1,1+epsilon]]) # (2,2)\n",
    "    eta = np.random.randn(D_in,N) # (D_in,N)\n",
    "    Y = np.dot(A_epsilon,X) + 0.01*eta # (D_out,N)\n",
    "    \n",
    "    # Randomly initializes the parameter matrix W\n",
    "    W = np.random.rand(2,2)\n",
    "    w0 = np.array([[1,0],[-1,0],[0,1],[0,-1]]) # (H,D_out)\n",
    "    w1 = np.dot(w0,W) # (H,D_in)\n",
    "    w2 = np.array([[1,0],[-1,0],[0,1],[0,-1]]) # (H,D_out)\n",
    "\n",
    "    for step in range(EPOCH): \n",
    "        #train_n = int(np.random.rand(1) * N)\n",
    "        train_n = int(0.7 * N) # the number of samples for training\n",
    "        idx = random.sample(list(range(N)),train_n) \n",
    "        X_training = X[:,idx] #(D_in,train_n)\n",
    "        Y_training = Y[:,idx] #(D_in,train_n)\n",
    "        \n",
    "        # forward propagation\n",
    "        z1 = np.dot(w1,X_training) #(H,train_n)\n",
    "        z2 = z1 #(H,train_n)\n",
    "        z2[z2<0] = 0\n",
    "        Y_pred_training = np.dot(w2.T,z2) #(D_out,train_n)\n",
    "\n",
    "        # the loss function\n",
    "        loss = np.linalg.norm((Y_pred_training - Y_training))**2/Y_pred_training.shape[1]\n",
    "        MSE.append(loss)\n",
    "\n",
    "        # back propagation\n",
    "        dl_dy_pred = (2/Y_training.shape[1]) * (Y_pred_training - Y_training) # (D_out,train_n)\n",
    "        dy_pred_dz2 = w2\n",
    "        dz2_dW = np.dot(w0,X_training)\n",
    "        dz2_dW[z1<0] = 0\n",
    "\n",
    "        dl_dW = np.dot(np.dot(dl_dy_pred,dz2_dW.T),dy_pred_dz2)\n",
    "        \n",
    "        # update the weight matrix\n",
    "        W -= LEARNING_RATE * dl_dW\n",
    "        w1 = np.dot(w0,W) \n",
    "        if step % 500 == 0 or step == EPOCH-1:\n",
    "            print('epsilon: 1e{}\\nepoch: {}\\n{}'.format(-k,step, W))\n",
    "        # test\n",
    "        if step == EPOCH-1:\n",
    "            n = 2000\n",
    "            np.random.seed(2)\n",
    "            x = np.random.randn(D_in,n)\n",
    "            e = np.random.randn(D_in,n)\n",
    "            y = np.dot(A_epsilon,x) + 0.01*e\n",
    "            \n",
    "            # forward propagation\n",
    "            z1 = np.dot(w1,x) #(H,train_n)\n",
    "            z2 = z1 #(H,train_n)\n",
    "            z2[z2<0] = 0\n",
    "            y_pred = np.dot(w2.T,z2) #(D_out,train_n)\n",
    "\n",
    "            # the loss function\n",
    "            loss = np.linalg.norm((y_pred - y))**2/y_pred.shape[1]\n",
    "            print('====='*5,'MSE: ')\n",
    "            print(loss)\n",
    "            \n",
    "    # plot the figure\n",
    "    plt.plot(range(EPOCH),MSE[EPOCH*k:EPOCH*(k+1)],'-',label='$\\epsilon$ = '+ str(epsilon))\n",
    "    #plt.ylim(0)\n",
    "    #plt.xlim(0)\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$MSE=\\frac{1}{n}\\Vert W_{x}X-Y\\Vert ^{2} $')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.title('Forward Problem')\n",
    "plt.savefig('nn(DP).pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcdbc0",
   "metadata": {},
   "source": [
    "### Inverse problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch version (without weight restriction)\n",
    "\n",
    "EPOCH = 2000\n",
    "LEARNING_RATE = 0.05\n",
    "N = 10000\n",
    "MSE = []\n",
    "\n",
    "# data generation\n",
    "for k in range(5):\n",
    "    torch.manual_seed(1)\n",
    "    X = torch.randn(2,N)\n",
    "    epsilon = 10**(-k)\n",
    "    A_epsilon = torch.Tensor([[1,1],[1,1+epsilon]]) \n",
    "    eta = torch.randn(2,N)\n",
    "    Y = torch.mm(A_epsilon,X) + 0.01*eta\n",
    "\n",
    "    net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(2,4),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(4,2)\n",
    "    )\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimzer = torch.optim.Adam(net.parameters(),lr = LEARNING_RATE)\n",
    "    #optimzer = torch.optim.SGD(net.parameters(),lr = LEARNING_RATE)\n",
    "\n",
    "# training\n",
    "    for epoch in range(EPOCH):\n",
    "        X_pred = net(Y.T)\n",
    "        loss = criterion(X_pred,X.T)\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "        MSE.append(loss.data.numpy())\n",
    "        # test\n",
    "        if epoch == EPOCH-1:\n",
    "            torch.manual_seed(2)\n",
    "            n = 2000\n",
    "            x = torch.randn(2,n)\n",
    "            e = torch.randn(2,n)\n",
    "            y = torch.mm(A_epsilon,x) + 0.01 * e\n",
    "            x_pred = net(y.T)\n",
    "            mse = criterion(x_pred,x.T)\n",
    "            print('epsilon: ',epsilon, 'MSE: ',mse)\n",
    "    plt.plot(range(EPOCH),MSE[EPOCH*k:EPOCH*(k+1)],'-',label='$\\epsilon$ = '+ str(epsilon))\n",
    "    #plt.ylim(0,3)\n",
    "    #plt.xlim(0)\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$MSE=\\frac{1}{n}\\Vert W_{y}Y-X\\Vert ^{2} $')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.title('Inverse Problem (Basic Matrix)')\n",
    "plt.savefig('nn(IPBMnoOutliers).pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90466a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy version (with weight restriction)\n",
    "torch.manual_seed(1)\n",
    "EPOCH = 3000\n",
    "LEARNING_RATE = 0.1\n",
    "N , D_in , H , D_out = 10000 , 2, 4 , 2 \n",
    "# N is the number of the samples，D_in is the dim of input，\n",
    "# H is the numeber of the notes in hidden layer,D_out is the dim of the output\n",
    "\n",
    "X = torch.randn(N,D_in) # input(N,D_in)\n",
    "MSE = []\n",
    "for k in range(5):\n",
    "    epsilon = 10**(-k)\n",
    "    A_epsilon = torch.Tensor([[1,1],[1,1+epsilon]]) # (2,2)\n",
    "    eta = torch.randn(N,D_in) # (N,D_in)\n",
    "    Y = X.mm(A_epsilon) + 0.01*eta # (N, D_out)\n",
    "    \n",
    "    # Randomly initializes the parameter matrix W\n",
    "    W = torch.rand(2,2)\n",
    "    w0 = torch.Tensor([[1,0],[-1,0],[0,1],[0,-1]]) # (H,D_out)\n",
    "    w1 = w0.mm(W) # (H,D_in)\n",
    "    w2 = torch.Tensor([[1,0],[-1,0],[0,1],[0,-1]]) # (H,D_out)\n",
    "\n",
    "    for step in range(EPOCH): \n",
    "        train_n = int(torch.rand(1) * N) # the number of samples for training\n",
    "        idx = random.sample(list(range(N)),train_n) \n",
    "        X_training = Y[idx] #(train_n,D_in)\n",
    "        Y_training = X[idx] #(train_n,D_in)\n",
    "        \n",
    "        # forward propagation\n",
    "        z1 = w1.mm(X_training.t()) #(H,train_n)\n",
    "        z2 = z1.clamp(min=0) #(H,train_n)\n",
    "        Y_pred_training = (w2.t().mm(z2)).t() #(train_n,D_out)\n",
    "\n",
    "        # the loss function\n",
    "        loss = torch.norm((Y_pred_training - Y_training)).pow(2)/Y_pred_training.shape[0]\n",
    "        MSE.append(loss)\n",
    "\n",
    "        # back propagation\n",
    "        dl_dy_pred = (2/Y_training.shape[0]) * (Y_pred_training - Y_training) # (train_n,D_out)\n",
    "        dy_pred_dz2 = w2\n",
    "        dz2_dW = w0.mm(X_training.t())\n",
    "        dz2_dW[z1.le(0)] = 0\n",
    "\n",
    "        dl_dW = dy_pred_dz2.t().mm(dz2_dW.mm(dl_dy_pred))\n",
    "        \n",
    "        # update the weight matrix\n",
    "        W -= LEARNING_RATE * dl_dW.t()\n",
    "        w1 = w0.mm(W) # (H,D_in)\n",
    "        if step % 500 == 0 or step == EPOCH-1:\n",
    "            print('epsilon: 1e{}\\nepoch: {}\\n{}'.format(-k,step, W))\n",
    "        # test\n",
    "        if step == EPOCH-1:\n",
    "            n = 2000\n",
    "            np.random.seed(2)\n",
    "            x = np.random.randn(D_in,n)\n",
    "            e = np.random.randn(D_in,n)\n",
    "            y = np.dot(A_epsilon,x) + 0.01 * e\n",
    "            \n",
    "            # forward propagation\n",
    "            z1 = np.dot(w1,y) #(H,train_n)\n",
    "            z2 = z1 #(H,train_n)\n",
    "            z2[z2<0] = 0\n",
    "            x_pred = np.dot(w2.T,z2) #(D_out,train_n)\n",
    "\n",
    "            # the loss function\n",
    "            loss = np.linalg.norm((x_pred - x))**2/x_pred.shape[1]\n",
    "            print('====='*5,'MSE: ')\n",
    "            print(loss)\n",
    "        \n",
    "    # plot the figure\n",
    "    plt.plot(range(EPOCH),MSE[EPOCH*k:EPOCH*(k+1)],'-',label='$\\epsilon$ = '+ str(epsilon))\n",
    "    plt.ylim(0,3)\n",
    "    plt.xlim(0)\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$MSE=\\frac{1}{n}\\Vert W_{y}Y-X\\Vert ^{2} $')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.title('Inverse Problem')\n",
    "plt.savefig('nn(IP).svg', format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5478a94",
   "metadata": {},
   "source": [
    "# Hilbert Linear Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1f4c8",
   "metadata": {},
   "source": [
    "## Condition number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilbert matrices and condition numbers\n",
    "Hilbert = {}\n",
    "cond = {}\n",
    "for dim in [2,4,9,16,25,32]:\n",
    "    H = 1. / (np.arange(1,dim+1) + np.arange(0,dim)[:, np.newaxis])\n",
    "    Hilbert['H'+str(dim)] = H\n",
    "    cond['H'+str(dim)] = np.linalg.cond(H)\n",
    "cond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf2851",
   "metadata": {},
   "source": [
    "## Tikhonov Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e32fad",
   "metadata": {},
   "source": [
    "### Optimal parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "np.random.seed(1)\n",
    "N = 10000 # number of data size\n",
    "LAMBDA = []\n",
    "X = np.random.randn(2,N)\n",
    "for key, value in Hilbert.items():\n",
    "    MSE = []\n",
    "    # create data sample\n",
    "    dim = value.shape[0]\n",
    "    X = np.random.randn(dim,N)\n",
    "    eta = np.random.randn(dim,N)\n",
    "    Y = np.dot(value,X) + 0.01 * eta\n",
    "    \n",
    "    for a in range(1,100001):\n",
    "        scale = 1e6\n",
    "        a = a/scale\n",
    "        # Tikhonov solution\n",
    "        X_tik = np.linalg.inv(np.dot(value.T,value) + a * np.eye(dim)).dot(value.T).dot(Y)\n",
    "        \n",
    "        # error\n",
    "        error = np.linalg.norm(X_tik-X)**2/X.shape[1]\n",
    "        MSE.append(error)\n",
    "        \n",
    "    Lambda = (MSE.index(min(MSE))+1)/scale    \n",
    "    LAMBDA.append(Lambda)\n",
    "    plt.plot((np.array(range(len(MSE)))+1)/scale,MSE,'-',label='dim = '+ str(dim))\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$MSE=\\frac{1}{n}\\Vert X_{tik}-X \\Vert$')\n",
    "    plt.xlabel(r'$\\lambda$')\n",
    "    plt.title('Optimal')\n",
    "    #plt.ylim(0)\n",
    "    plt.show()\n",
    "    print('dim = ', str(dim),' MSE:',min(MSE),' lambda:',Lambda ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "N = 2000 # number of data size\n",
    "MSE = []\n",
    "for dim,a in zip([2,4,9,16,25,32],LAMBDA):\n",
    "    H = 1. / (np.arange(1,dim+1) + np.arange(0,dim)[:, np.newaxis])\n",
    "    # create data sample\n",
    "    dim = H.shape[0]\n",
    "    torch.manual_seed(2)\n",
    "    X = np.random.randn(dim,N)\n",
    "    eta = np.random.randn(dim,N)\n",
    "    Y = np.dot(H,X) + 0.01 * eta\n",
    "\n",
    "    # Tikhonov solution\n",
    "    X_tik = np.linalg.inv(np.dot(H.T,H) + a * np.eye(dim)).dot(H.T).dot(Y)\n",
    "        \n",
    "    # error\n",
    "    error = np.linalg.norm(X_tik-X)**2/X.shape[1]\n",
    "    MSE.append(error)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4693af",
   "metadata": {},
   "source": [
    "### L-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df552349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "np.random.seed(1)\n",
    "N = 10000 # number of data size\n",
    "LAMBDA = []\n",
    "for key, value in Hilbert.items():\n",
    "    solution = []\n",
    "    residual = []\n",
    "    # create data sample\n",
    "    dim = value.shape[0]\n",
    "    X = np.random.randn(dim,N)\n",
    "    eta = np.random.randn(dim,N)\n",
    "    Y = np.dot(value,X) + 0.01 * eta\n",
    "    \n",
    "    for a in range(1,1001):\n",
    "        scale = 1e2\n",
    "        a = a/scale\n",
    "        # Tikhonov solution\n",
    "        X_tik = np.dot(np.dot(np.linalg.inv(np.dot(value.T,value) + a * np.eye(dim)),value.T),Y)\n",
    "        \n",
    "        # norm\n",
    "        X_tik_norm = np.linalg.norm(X_tik)\n",
    "        residual_norm = np.linalg.norm(np.dot(value,X_tik) - Y)\n",
    "        solution.append(X_tik_norm)\n",
    "        residual.append(residual_norm)\n",
    "        \n",
    "    # L-curve figure\n",
    "    plt.plot(residual,solution,'.',label='dim = '+ str(dim))\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$\\log\\Vert X_{Tik} \\Vert$')\n",
    "    plt.xlabel(r'$\\log\\Vert AX_{Tik}-Y^{\\eta} \\Vert$')\n",
    "    plt.title('Discrete L-curve for Tikhonov regularization')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.show()\n",
    "\n",
    "    Lambda = (curvature(residual,solution)+1)/scale\n",
    "    LAMBDA.append(Lambda)\n",
    "    X_tik_Lambda = np.dot(np.dot(np.linalg.inv(np.dot(value.T,value) + Lambda * np.eye(dim)),value.T),Y)\n",
    "    MSE = np.linalg.norm(X_tik_Lambda-X)**2 / Y.shape[1]\n",
    "    \n",
    "    print('dim: ',dim,'lambda: ',Lambda, ' MSE: ',MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8483d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "N = 2000 # number of data size\n",
    "MSE = []\n",
    "for dim,a in zip([2,4,9,16,25,32],LAMBDA):\n",
    "    H = 1. / (np.arange(1,dim+1) + np.arange(0,dim)[:, np.newaxis])\n",
    "    # create data sample\n",
    "    dim = H.shape[0]\n",
    "    torch.manual_seed(2)\n",
    "    X = np.random.randn(dim,N)\n",
    "    eta = np.random.randn(dim,N)\n",
    "    Y = np.dot(H,X) + 0.01 * eta\n",
    "\n",
    "    # Tikhonov solution\n",
    "    X_tik = np.linalg.inv(np.dot(H.T,H) + a * np.eye(dim)).dot(H.T).dot(Y)\n",
    "        \n",
    "    # error\n",
    "    error = np.linalg.norm(X_tik-X)**2/X.shape[1]\n",
    "    MSE.append(error)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f53e0",
   "metadata": {},
   "source": [
    "### GCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc1068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "np.random.seed(1)\n",
    "N = 10000 # number of data size\n",
    "LAMBDA = []\n",
    "for key, value in Hilbert.items():\n",
    "    GCV = []\n",
    "    # create data sample\n",
    "    dim = value.shape[0]\n",
    "    X = np.random.randn(dim,N)\n",
    "    eta = np.random.randn(dim,N)\n",
    "    Y = np.dot(value,X) + 0.01 * eta\n",
    "    \n",
    "    for a in range(1000):\n",
    "        scale = 1e5\n",
    "        a = a / scale\n",
    "        # Tikhonov solution\n",
    "        X_tik = np.dot(np.dot(np.linalg.inv(np.dot(value.T,value) + a * np.eye(dim)),value.T),Y)\n",
    "        \n",
    "        # GCV\n",
    "        upper = np.linalg.norm(np.dot(value,X_tik)-Y)**2\n",
    "        lower = np.trace(np.eye(dim) - np.dot(np.dot(value,np.linalg.inv(np.dot(value.T,value)+ a * np.eye(dim))),value.T))**2\n",
    "        gcv = upper/lower\n",
    "        GCV.append(gcv)\n",
    "    \n",
    "    plt.plot(np.array(range(len(GCV)))/scale,GCV,'-',label='dim = '+ str(dim))\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$GCV(\\lambda)$')\n",
    "    plt.xlabel(r'$\\lambda$')\n",
    "    plt.title('GCV')\n",
    "    plt.ylim(0,10)\n",
    "    \n",
    "    Lambda = (GCV.index(min(GCV))+1)/scale\n",
    "    LAMBDA.append(Lambda)\n",
    "    X_tik_Lambda = np.dot(np.dot(np.linalg.inv(np.dot(value.T,value) + Lambda * np.eye(dim)),value.T),Y)\n",
    "    MSE = np.linalg.norm(X_tik_Lambda-X)**2 / Y.shape[1]\n",
    "    print('dim:',dim,' GCV:',min(GCV),' lambda:',GCV.index(min(GCV))/scale ,' MSE: ',MSE,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "N = 2000 # number of data size\n",
    "MSE = []\n",
    "for dim,a in zip([2,4,9,16,25,32],LAMBDA):\n",
    "    H = 1. / (np.arange(1,dim+1) + np.arange(0,dim)[:, np.newaxis])\n",
    "    # create data sample\n",
    "    dim = H.shape[0]\n",
    "    torch.manual_seed(2)\n",
    "    X = np.random.randn(dim,N)\n",
    "    eta = np.random.randn(dim,N)\n",
    "    Y = np.dot(H,X) + 0.01 * eta\n",
    "\n",
    "    # Tikhonov solution\n",
    "    X_tik = np.linalg.inv(np.dot(H.T,H) + a * np.eye(dim)).dot(H.T).dot(Y)\n",
    "        \n",
    "    # error\n",
    "    error = np.linalg.norm(X_tik-X)**2/X.shape[1]\n",
    "    MSE.append(error)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7d5c7",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67063f3",
   "metadata": {},
   "source": [
    "### Forward problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1695f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12704/4111165645.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# create data sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# torch version (without weight restriction)\n",
    "\n",
    "EPOCH = 500\n",
    "LEARNING_RATE = 0.005\n",
    "N = 10000\n",
    "    \n",
    "MSE = []\n",
    "for k,dim in enumerate([2,4,9,16,25,32]):\n",
    "    H = 1. / (np.arange(1,dim+1) + np.arange(0,dim)[:, np.newaxis])\n",
    "    H = torch.tensor(H).float()\n",
    "    # create data sample\n",
    "    dim = H.shape[0]\n",
    "    torch.manual_seed(1)\n",
    "    X = torch.randn(dim,N).float()\n",
    "    eta = torch.randn(dim,N).float()\n",
    "    Y = torch.mm(H,X) + 0.01 * eta\n",
    "\n",
    "    # batch_n = 100\n",
    "    hidden_layer = dim * 2\n",
    "    input_data = dim\n",
    "    output_data = dim\n",
    "\n",
    "    net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_data,hidden_layer),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_layer,output_data)\n",
    "    )\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimzer = torch.optim.Adam(net.parameters(),lr = LEARNING_RATE)\n",
    "\n",
    "# training\n",
    "    for epoch in range(EPOCH):\n",
    "        Y_pred = net(X.T)\n",
    "        loss = criterion(Y_pred,Y.T)\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "        MSE.append(loss.data.numpy())\n",
    "        \n",
    "        # test\n",
    "        if epoch == EPOCH-1:\n",
    "            torch.manual_seed(2)\n",
    "            n = 2000\n",
    "            x = torch.randn(dim,n)\n",
    "            e = torch.randn(dim,n)\n",
    "            y = torch.mm(H,x) + 0.01 * e\n",
    "            y_pred = net(x.T)\n",
    "            mse = criterion(y_pred,y.T)\n",
    "            print('dim: ',dim, 'MSE: ',mse)         \n",
    "    plt.plot(range(EPOCH),MSE[EPOCH*k:EPOCH*(k+1)],'-',label= 'dim: '+ str(dim))\n",
    "    #plt.ylim(0,3)\n",
    "    #plt.xlim(0)\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$MSE=\\frac{1}{n}\\Vert W_{x}X-Y\\Vert ^{2} $')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.title('Forward Problem (Hilbert Matrix)')\n",
    "plt.savefig('nn(DPHMnoOutlier).pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9cb20",
   "metadata": {},
   "source": [
    "### Inverse problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e37e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch version (without weight restriction)\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import normal\n",
    "\n",
    "EPOCH = 1000\n",
    "LEARNING_RATE = 0.05\n",
    "N = 10000\n",
    "std = 10\n",
    "MSE = []\n",
    "for k,dim in enumerate([2,4,9,16,25,32]):\n",
    "    H = 1. / (np.arange(1,dim+1) + np.arange(0,dim)[:, np.newaxis])\n",
    "    H = torch.tensor(H).float()\n",
    "    # create data sample\n",
    "    dim = H.shape[0]\n",
    "    torch.manual_seed(1)\n",
    "    X = torch.randn(dim,N).float()\n",
    "    eta = torch.empty(dim,N).normal_(mean=0,std=std)\n",
    "    Y = torch.mm(H,X) + 0.01 * eta\n",
    "\n",
    "    # batch_n = 100\n",
    "    hidden_layer = dim * 2\n",
    "    input_data = dim\n",
    "    output_data = dim\n",
    "\n",
    "    net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_data,hidden_layer),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_layer,output_data)\n",
    "    )\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimzer = torch.optim.Adam(net.parameters(),lr = LEARNING_RATE)\n",
    "\n",
    "# training\n",
    "    for epoch in range(EPOCH):\n",
    "        X_pred = net(Y.T)\n",
    "        loss = criterion(X_pred,X.T)\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "        MSE.append(loss.data.numpy())\n",
    "        if epoch == EPOCH-1:\n",
    "            print(loss)  \n",
    "        # test\n",
    "        if epoch == EPOCH-1:\n",
    "            torch.manual_seed(2)\n",
    "            n = 2000\n",
    "            x = torch.randn(dim,n)\n",
    "            e = torch.empty(dim,n).normal_(mean=0,std=std)\n",
    "            y = torch.mm(H,x) + 0.01 * e\n",
    "            x_pred = net(y.T)\n",
    "            mse = criterion(x_pred,x.T)\n",
    "            print('dim: ',dim, 'MSE: ',mse)  \n",
    "    plt.plot(range(EPOCH),MSE[EPOCH*k:EPOCH*(k+1)],'-',label= 'dim: '+ str(dim))\n",
    "    #plt.ylim(0,3)\n",
    "    #plt.xlim(0)\n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$MSE=\\frac{1}{n}\\Vert W_{y}Y-X\\Vert ^{2} $')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.title('Inverse Problem (Hilbert Matrix)')\n",
    "plt.savefig('nn(IPHMnoOutlier).pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc9560",
   "metadata": {},
   "source": [
    "### ISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrinkage(x, theta):\n",
    "    max = np.abs(x) - theta\n",
    "    max[max<0] = 0\n",
    "    return np.sign(x) * max\n",
    "\n",
    "def ista(Y, X, A, max_iter, eps,a=None, L=None):\n",
    "\n",
    "    eig, eig_vector = np.linalg.eig(A.T.dot(A))\n",
    "    if L is not None:\n",
    "        assert L > np.max(eig)\n",
    "    if L == None:\n",
    "        L = np.max(eig)+0.1\n",
    "    if a == None:\n",
    "        a = 0.0001 * L\n",
    "    del eig, eig_vector\n",
    "    \n",
    "    B = (1/L)*A.T\n",
    "\n",
    "    ista_err = []\n",
    "    X_old = np.zeros((A.shape[1], Y.shape[1]))\n",
    "    for i in range(max_iter):\n",
    "\n",
    "        X_new = shrinkage(X_old-B.dot(A.dot(X_old)-Y),a/L)\n",
    "\n",
    "        if np.sum(np.abs(X_new - X_old))/Y.shape[1] < eps:\n",
    "            break\n",
    "        X_old = X_new\n",
    "        error = np.linalg.norm(X_old - X)**2/Y.shape[1]\n",
    "        ista_err.append(error)\n",
    "        \n",
    "    return X_new, ista_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "N = 10000 # number of data size\n",
    "max_iter = 5000\n",
    "\n",
    "for key, value in Hilbert.items():\n",
    "    solution = []\n",
    "    residual = []\n",
    "\n",
    "    # create data sample\n",
    "    dim = value.shape[0]\n",
    "    X = np.random.randn(dim,N)\n",
    "    eta = np.random.randn(dim,N)\n",
    "    Y = np.dot(value,X) + 0.01 * eta\n",
    "\n",
    "    X_ISTA, ISTAerror = ista(Y, X, value, max_iter=max_iter, eps=1e-6)\n",
    "    #plt.subplot(2, 1, 2)\n",
    "    plt.plot(range(len(ISTAerror)),ISTAerror, '-', label='dim = '+ str(dim))\n",
    "    plt.legend()\n",
    "    #plt.ylim(0,0.0001)\n",
    "    plt.ylabel('MSE')\n",
    "    #plt.yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "    print('dim:',dim,' iter:',len(ISTAerror), ' MSE:',ISTAerror[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82bfba",
   "metadata": {},
   "source": [
    "### LISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198df418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LISTA(nn.Module):\n",
    "    def __init__(self, n, m, W_e, max_iter, theta, L=None):\n",
    "        \n",
    "        super(LISTA, self).__init__()\n",
    "        self._W = nn.Linear(in_features=n, out_features=m, bias=False)\n",
    "        self._S = nn.Linear(in_features=m, out_features=n, bias=False)\n",
    "        self.shrinkage = nn.Softshrink(theta)\n",
    "        self.theta = theta\n",
    "        self.max_iter = max_iter\n",
    "        self.A = W_e\n",
    "        self.L = L\n",
    "        \n",
    "    # weights initialization based on the dictionary\n",
    "    def weights_init(self):\n",
    "        L = self.L\n",
    "        A = self.A.cpu().numpy()\n",
    "        S = torch.from_numpy(np.eye(A.shape[1]) - (1/L)*np.dot(A.T, A))\n",
    "        S = S.float().to(device)\n",
    "        W = torch.from_numpy((1/L)*A.T)\n",
    "        W = W.float().to(device)\n",
    "        \n",
    "        self._S.weight = nn.Parameter(S)\n",
    "        self._W.weight = nn.Parameter(W)\n",
    "\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = self.shrinkage(self._W(y))\n",
    "\n",
    "        if self.max_iter == 1 :\n",
    "            return x\n",
    "\n",
    "        for iter in range(self.max_iter):\n",
    "            x = self.shrinkage(self._W(y) + self._S(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_lista(Y, X, dictionary, lr, max_iter=15,epoch=100,L=None, batch_size=None,eps=1e-6):\n",
    "    \n",
    "    eig, eig_vector = np.linalg.eig(dictionary.T.dot(dictionary))\n",
    "    if L is not None:\n",
    "        assert L > torch.max(eig)\n",
    "    if L == None:\n",
    "        L = np.max(eig)+0.1\n",
    "    del eig, eig_vector\n",
    "\n",
    "    n, m = dictionary.shape\n",
    "    n_samples = Y.shape[1]\n",
    "    #steps_per_epoch = n_samples // batch_size\n",
    "    \n",
    "    # convert the data into tensors\n",
    "    Y = torch.from_numpy(Y)\n",
    "    Y = Y.float().to(device)\n",
    "    X = torch.from_numpy(X)\n",
    "    X = X.float().to(device)\n",
    "    \n",
    "    W_d = torch.from_numpy(dictionary)\n",
    "    W_d = W_d.float().to(device)\n",
    "\n",
    "    net = LISTA(n, m, W_d, max_iter=max_iter, L=L, theta=1/L)\n",
    "    net = net.float().to(device)\n",
    "    net.weights_init()\n",
    "\n",
    "    # build the optimizer and criterion\n",
    "    criterion1 = nn.MSELoss()\n",
    "    #criterion2 = nn.L1Loss()\n",
    "    #all_zeros = torch.zeros(batch_size, m).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    loss_list = []\n",
    "    X_h_old = torch.zeros((n, n_samples)).to(device)\n",
    "    for e in range(epoch):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # get the outputs\n",
    "        X_h = net(Y.T).T\n",
    "        #Y_h = torch.mm(X_h, W_d.T)\n",
    "        if torch.sum(torch.abs(X_h - X_h_old))/n_samples < eps:\n",
    "            break\n",
    "        \n",
    "        # compute the losss\n",
    "        loss1 = criterion1(X.float(), X_h.float())\n",
    "        #loss2 = a * criterion2(X_h.float(), all_zeros.float())\n",
    "        #loss = loss1 + loss2\n",
    "        loss = loss1\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        X_h_old = X_h  \n",
    "  \n",
    "        with torch.no_grad():\n",
    "          loss_list.append(loss.cpu().data.numpy()) \n",
    "    #for epoch in range(50):\n",
    "     # index_samples = np.random.choice(a=n_samples, size=n_samples, replace=False, p=None)\n",
    "      #Y_shuffle = Y[:,index_samples]\n",
    "      #X_shuffle = X[:,index_samples]\n",
    "      #for step in range(steps_per_epoch):\n",
    "        #Y_batch = Y_shuffle[:,step*batch_size:(step+1)*batch_size]\n",
    "        #X_batch = X_shuffle[:,step*batch_size:(step+1)*batch_size]\n",
    "        #optimizer.zero_grad()\n",
    "    \n",
    "        # get the outputs\n",
    "        #X_h = net(Y_batch.T).T\n",
    "        #Y_h = torch.mm(X_h, W_d.T)\n",
    "    \n",
    "        # compute the losss\n",
    "        #loss1 = criterion1(X_batch.float(), X_h.float())\n",
    "        #loss2 = a * criterion2(X_h.float(), all_zeros.float())\n",
    "        #loss = loss1 + loss2\n",
    "        #loss = loss1\n",
    "            \n",
    "        #loss.backward()\n",
    "        #optimizer.step()  \n",
    "  \n",
    "        #with torch.no_grad():\n",
    "          #loss_list.append(loss.cpu().data.numpy()) \n",
    "\n",
    "    plt.plot(range(len(loss_list)),loss_list,'-',label='$\\dim$ = '+ str(m)) \n",
    "    plt.legend()\n",
    "    plt.ylabel(r'$MSE=\\frac{1}{n}\\Vert W_{y}Y-X\\Vert ^{2} $')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.show()\n",
    "    print('Epoch: ',len(loss_list),'MSE: ',loss_list[-1])       \n",
    "            \n",
    "    return net, loss_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import orth\n",
    "\n",
    "N = 10000 # number of data size\n",
    "for key, value in Hilbert.items():\n",
    "    solution = []\n",
    "    residual = []\n",
    "    # create data sample\n",
    "    dim = value.shape[0]\n",
    "    np.random.seed(1)\n",
    "    X = np.random.randn(dim,N)\n",
    "    eta = np.random.randn(dim,N)\n",
    "    Y = np.dot(value,X) + 0.01 * eta\n",
    "    lista, lista_err = train_lista(Y,X, value,max_iter=1, lr=0.05,epoch=5000)\n",
    "\n",
    "# Test stage\n",
    "# generate sparse signal Z and measurement X\n",
    "    n = 2000\n",
    "    np.random.seed(2)\n",
    "    X = np.random.randn(dim,n)\n",
    "    eta = np.random.randn(dim,n)\n",
    "    Y = np.dot(value,X) + 0.01 * eta\n",
    "    X = torch.from_numpy(X).float().to(device)\n",
    "\n",
    "    X_LISTA = lista(torch.from_numpy(Y.T).float().to(device)).T\n",
    "    Loss = nn.MSELoss()\n",
    "    loss = Loss(X_LISTA, X)\n",
    "    print(loss.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dad958",
   "metadata": {},
   "source": [
    "# DnCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise\n",
    "def addnoise(img,types,n,mean=0,sd=1,lam=1,multi=False):\n",
    "    image = img.clone()\n",
    "    #image = img\n",
    "    c,h,w = image.shape\n",
    "    \n",
    "    if types == 'original':\n",
    "        pass\n",
    "    \n",
    "    elif types == 'saltpepper':\n",
    "        n = n // 2\n",
    "        for k in range(n):\n",
    "            i = int(np.random.random() * image.shape[1])\n",
    "            j = int(np.random.random() * image.shape[2])\n",
    "            if image.ndim == 2:\n",
    "                image[j,i] = 1\n",
    "            elif img.ndim == 3:\n",
    "                image[0,j,i]= 1\n",
    "                image[1,j,i]= 1\n",
    "                image[2,j,i]= 1\n",
    "        for k in range(n):\n",
    "            i = int(np.random.random() * image.shape[1])\n",
    "            j = int(np.random.random() * image.shape[2])\n",
    "            if image.ndim == 2:\n",
    "                image[j,i] = 0\n",
    "            elif img.ndim == 3:\n",
    "                image[0,j,i]= 0\n",
    "                image[1,j,i]= 0\n",
    "                image[2,j,i]= 0\n",
    "                \n",
    "    else:\n",
    "        mask = np.random.choice((0,1),size=(c,h,w),p=[1-n/h/w,n/h/w])\n",
    "        \n",
    "        if types == 'gaussian':\n",
    "            noise = np.random.normal(loc=mean, scale=sd, size=(c,h,w))\n",
    "\n",
    "        if types == 'poisson':\n",
    "            noise = np.random.poisson(lam=lam, size=(c,h,w))\n",
    "\n",
    "        if types == 'uniform':\n",
    "            noise = np.random.random(size=(c,h,w))\n",
    "        \n",
    "        if types == 'exponential':\n",
    "            noise = np.random.exponential(scale=lam,size=(c,h,w))\n",
    "            \n",
    "        if types == 'lognormal':\n",
    "            noise = np.random.lognormal(mean=mean,sigma=sd,size=(c,h,w))\n",
    "            \n",
    "        if types == 'rayleigh':\n",
    "            noise = np.random.rayleigh(scale=lam,size=(c,h,w))\n",
    "      \n",
    "        noise = noise * mask\n",
    "        \n",
    "        if multi == False:\n",
    "            image = image + noise\n",
    "        if multi == True:\n",
    "            image = image + image * noise\n",
    "  \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2573e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset    \n",
    "def dataset(data,size,seed=0,noise_type='gaussian',noise_num=250,multi=False):\n",
    "    img = []\n",
    "    img_gt = []\n",
    "    img_noise = []\n",
    "    if size == 'all':\n",
    "        for k,(j,_) in enumerate(data):\n",
    "            img_gt.append(j)\n",
    "            imgnoise = addnoise(j,types=noise_type,n=noise_num,multi=multi)\n",
    "            img_noise.append(imgnoise)\n",
    "        else:\n",
    "            np.random.seed(seed)\n",
    "            index = np.random.randint(0,len(data),size=size)\n",
    "    for i in index:\n",
    "        img.append(data[i])\n",
    "        \n",
    "    for k,(j,_) in enumerate(img):\n",
    "        img_gt.append(j)\n",
    "        imgnoise = addnoise(j,types=noise_type,n=noise_num,multi=multi)\n",
    "        img_noise.append(imgnoise)\n",
    "    return img_gt,img_noise    \n",
    "  \n",
    "# show image\n",
    "def imgshow(data,fig_num,fig_size):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    for i,j in enumerate(data):\n",
    "        ax = plt.subplot(math.ceil(len(data)/50), 5, i+1 ) # 行，列，索引（需从1开始）\n",
    "        to_img = transforms.ToPILImage()\n",
    "        n = to_img(j)\n",
    "        ax.imshow(n) \n",
    "        #ax.set_title(name[i])\n",
    "        ax.set_xticks([])  \n",
    "        ax.set_yticks([])\n",
    "        if i+1 == fig_num:\n",
    "            break\n",
    "    return plt.show()  \n",
    "  \n",
    "\n",
    "# create DnCNN\n",
    "class DnCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, depth=12, n_channels=64, image_channels=3, use_bnorm=True, kernel_size=3):\n",
    "        super(DnCNN, self).__init__()\n",
    "        \n",
    "        kernel_size = 3  \n",
    "        padding = 1  \n",
    "        layers = [] \n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels=image_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding, bias=True))\n",
    "        layers.append(nn.ReLU())\n",
    "        for _ in range(depth-2):\n",
    "            layers.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding, bias=True))\n",
    "            layers.append(nn.BatchNorm2d(n_channels, eps=0.0001, momentum = 0.9))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Conv2d(in_channels=n_channels, out_channels=image_channels, kernel_size=kernel_size, padding=padding, bias=True))\n",
    "        self.dncnn = nn.Sequential(*layers).to(device)\n",
    "        self._initialize_weights() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        y = x.to(device)\n",
    "        out = self.dncnn(x).to(device)\n",
    "        return y-out\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.orthogonal_(m.weight)\n",
    "                print('init weight')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "dncnn = DnCNN()\n",
    "print(dncnn)\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    " \n",
    "def create_window(window_size, channel=3):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    " \n",
    "# SSIM\n",
    "def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):\n",
    "    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
    "    if val_range is None:\n",
    "        if torch.max(img1) > 128:\n",
    "            max_val = 255\n",
    "        else:\n",
    "            max_val = 1\n",
    " \n",
    "        if torch.min(img1) < -0.5:\n",
    "            min_val = -1\n",
    "        else:\n",
    "            min_val = 0\n",
    "        L = max_val - min_val\n",
    "    else:\n",
    "        L = val_range\n",
    " \n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    " \n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    " \n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    " \n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    " \n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    " \n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    " \n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    " \n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    " \n",
    "    if full:\n",
    "        return ret, cs\n",
    "    return ret\n",
    " \n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size=11, size_average=True, val_range=None):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.val_range = val_range\n",
    " \n",
    "        # Assume 1 channel for SSIM\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size)\n",
    " \n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    " \n",
    "        if channel == self.channel and self.window.dtype == img1.dtype:\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel).to(img1.device).type(img1.dtype)\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "        ssimloss = ssim(img1, img2, window=window, window_size=self.window_size, size_average=self.size_average)\n",
    " \n",
    "        return ssimloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d62637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_training(data_noise,data_gt,test_noise,test_gt,epoch,lr,criterion,name,net=dncnn,optimizer='adam',fig_num=range(0,100,9),loss_min=0):\n",
    "\n",
    "    Epoch = epoch\n",
    "    LOSS = []\n",
    "    if criterion == 'MSE':\n",
    "        Criterion = nn.MSELoss()\n",
    "    if criterion == 'SSIM':\n",
    "        Criterion = SSIM()\n",
    "        criterion = '1-SSIM'\n",
    "    if criterion == 'PSNR':\n",
    "        Criterion = PSRN()\n",
    "        criterion = '1-PSNR'\n",
    "    Criterion = Criterion.to(device)\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "    print(\"Training loop:\")\n",
    "    for idx in range(0,Epoch):\n",
    "        for input, target in zip(data_noise,data_gt):\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            input = input.float().to(device)\n",
    "            target = target.float().to(device)\n",
    "            #output = input - net(input)\n",
    "            output = net(input)\n",
    "            noise = (input - target)\n",
    "            if criterion == 'MSE':\n",
    "                loss = Criterion(output,noise)\n",
    "            if criterion == '1-SSIM' or criterion == '1-PSRN':\n",
    "                loss = 1 - Criterion(output,noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()   \n",
    "            #if idx % 500 == 0:\n",
    "                #print(\"Epoch {: >8} Loss: {}\".format(idx, loss.data.numpy()))\n",
    "            #print(\"Epoch {: >8} Loss: {}\".format(idx, loss.cpu()))\n",
    "            LOSS.append(loss.cpu().data.numpy())\n",
    "    \n",
    "            # show the training image: ground-truth image, noisy image, denoising image, predicted noise, ground-truth noise\n",
    "            #if idx in fig_num:\n",
    "            #  for j,k in zip(data_noise,data_gt):\n",
    "            #    to_img = transforms.ToPILImage()\n",
    "            #    j = j.float().to(device)\n",
    "            #    k = k.float().to(device)\n",
    "            #    #pred = j - net(j)   # the denoising image\n",
    "            #    pred = net(j)  # output is the noise\n",
    "            #    plt.figure(figsize=(20,100))\n",
    "            #    for n,m,p in zip(pred,j,k):\n",
    "            #      n = n.to(device)\n",
    "            #      ax = plt.subplot(1, 5, 1) \n",
    "            #      img = to_img(p)\n",
    "            #      plt.imshow(img)\n",
    "            #      ax.set_xticks([]) \n",
    "            #      ax.set_yticks([])\n",
    "            #      ax = plt.subplot(1, 5, 2)\n",
    "            #      img = to_img(m)\n",
    "            #      plt.imshow(img)\n",
    "            #      ax.set_xticks([]) \n",
    "            #      ax.set_yticks([])\n",
    "            #      ax = plt.subplot(1, 5, 3)\n",
    "            #      img = to_img(m-n)\n",
    "            #      plt.imshow(img)\n",
    "            #      ax.set_xticks([]) \n",
    "            #      ax.set_yticks([])\n",
    "            #      ax = plt.subplot(1, 5, 4)\n",
    "            #      img = to_img(n)\n",
    "            #      plt.imshow(img)\n",
    "            #      ax.set_xticks([]) \n",
    "            #      ax.set_yticks([])\n",
    "            #      ax = plt.subplot(1, 5, 5)\n",
    "            #      img = to_img(m-p)\n",
    "            #      plt.imshow(img)\n",
    "            #      ax.set_xticks([]) \n",
    "            #      ax.set_yticks([])\n",
    "            #      break\n",
    "            #    plt.show()\n",
    "            #    break\n",
    "            # the stopping strategy\n",
    "            if loss < loss_min:\n",
    "                break\n",
    "    path = name + '.pkl'\n",
    "    torch.save(net,path) \n",
    "    plt.plot(LOSS,'-')\n",
    "    plt.ylabel(criterion)\n",
    "    plt.xlabel('Epoch')\n",
    "    #plt.yscale('log')\n",
    "    plt.show()\n",
    "    mse = []\n",
    "  \n",
    "    # show the last epoch result of training set\n",
    "    for input,target in zip(data_noise,data_gt):\n",
    "        input = input.float().to(device)\n",
    "        target = target.float().to(device)\n",
    "        output = net(input)\n",
    "        denoising = input - output\n",
    "        mse.append(Criterion(denoising,target).cpu().data.numpy())\n",
    "    MSE = np.mean(mse)\n",
    "    PRSN = round(20 * math.log10(1 / math.sqrt(MSE)),3)\n",
    "    print('training PRSN: ',PRSN)\n",
    "  \n",
    "    # show the performance of test set\n",
    "    for input,target in zip(test_noise,test_gt):\n",
    "        input = input.float().to(device)\n",
    "        target = target.float().to(device)\n",
    "        output = net(input)\n",
    "        denoising = input - output\n",
    "        mse.append(Criterion(denoising,target).cpu().data.numpy())\n",
    "    MSE = np.mean(mse)\n",
    "    PRSN = round(20 * math.log10(1 / math.sqrt(MSE)),3)\n",
    "    print('test PRSN: ',PRSN)\n",
    "  \n",
    "    # show the last epoch training image of training set\n",
    "    for j,k in zip(data_noise,data_gt):\n",
    "        to_img = transforms.ToPILImage()\n",
    "        j = j.float().to(device)\n",
    "        k = k.float().to(device)\n",
    "        #pred = j - net(j)\n",
    "        pred = net(j)\n",
    "        plt.figure(figsize=(20,100))\n",
    "        count = 25\n",
    "        i = 0\n",
    "            for n,m,p in zip(pred,j,k):\n",
    "                i += 1\n",
    "                if i == count:\n",
    "                    n = n.to(device)\n",
    "                    ax = plt.subplot(1, 5, 1) \n",
    "                    img = to_img(p)\n",
    "                    plt.imshow(img)\n",
    "                    ax.set_xticks([]) \n",
    "                    ax.set_yticks([])\n",
    "                    ax = plt.subplot(1, 5, 2)\n",
    "                    img = to_img(m)\n",
    "                    plt.imshow(img)\n",
    "                    ax.set_xticks([])\n",
    "                    ax.set_yticks([])\n",
    "                    ax = plt.subplot(1, 5, 3)\n",
    "                    img = to_img(m-n)\n",
    "                    plt.imshow(img)\n",
    "                    ax.set_xticks([]) \n",
    "                    ax.set_yticks([])\n",
    "                    ax = plt.subplot(1, 5, 4)\n",
    "                    img = to_img(n)\n",
    "                    plt.imshow(img)\n",
    "                    ax.set_xticks([]) \n",
    "                    ax.set_yticks([])\n",
    "                    ax = plt.subplot(1, 5, 5)\n",
    "                    img = to_img(m-p)\n",
    "                    plt.imshow(img)\n",
    "                    ax.set_xticks([]) \n",
    "                    ax.set_yticks([])\n",
    "                    plt.show()\n",
    "                    break        \n",
    "    print('========'*5)\n",
    "  \n",
    "    # show the predict performance of test set\n",
    "    for j,k in zip(test_noise,test_gt):\n",
    "    to_img = transforms.ToPILImage()\n",
    "    j = j.float().to(device)\n",
    "    k = k.float().to(device)\n",
    "    #pred = j - net(j)\n",
    "    pred = net(j)\n",
    "    plt.figure(figsize=(20,100))\n",
    "    count = 10\n",
    "    i = 0\n",
    "    for n,m,p in zip(pred,j,k):\n",
    "        i += 1\n",
    "        if i == count:\n",
    "            n = n.to(device)\n",
    "            ax = plt.subplot(1, 5, 1) \n",
    "            img = to_img(p)\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 5, 2)\n",
    "            img = to_img(m)\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 5, 3)\n",
    "            img = to_img(m-n)\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 5, 4)\n",
    "            img = to_img(n)\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 5, 5)\n",
    "            img = to_img(m-p)\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            break\n",
    "    return LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image and add addictive Gaussian noise\n",
    "train_gt_gau,train_noise_gau = dataset(train_set,size=1024,seed=123,noise_type='gaussian',noise_num=1000,multi=False)\n",
    "test_gt_gau,test_noise_gau = dataset(test_set,size=256,seed=123,noise_type='gaussian',noise_num=1000,multi=False)\n",
    "\n",
    "batch_size = 64\n",
    "train_noise_gau_batch = DataLoader(train_noise_gau, batch_size=batch_size, shuffle=False)\n",
    "train_gt_gau_batch = DataLoader(train_gt_gau, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_noise_gau_batch = DataLoader(test_noise_gau, batch_size=batch_size, shuffle=False,num_workers=0)\n",
    "test_gt_gau_batch = DataLoader(test_gt_gau, batch_size=batch_size, shuffle=False,num_workers=0)\n",
    "\n",
    "# DnCNN training\n",
    "a=net_training(train_noise_gau_batch,train_gt_gau_batch,test_noise_gau_batch,test_gt_gau_batch,net=dncnn,\n",
    "        epoch=200,optimizer='adam',criterion='MSE',lr=0.0001,fig_num=range(0,200,9),loss_min=1e-6,\n",
    "        name='rayleighAdd1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a79c6",
   "metadata": {},
   "source": [
    "# ISTA-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6832c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ISTA-Net Block\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    def __init__(self,n_channels=128, image_channels=3,kernel_size=3):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        fw_layers = []\n",
    "        bw_layers = []\n",
    " \n",
    "        self.lambda_step = nn.Parameter(torch.Tensor([1]))\n",
    "        self.soft_thr = nn.Parameter(torch.Tensor([0.5]))\n",
    "\n",
    "        fw_layers.append(nn.Conv2d(in_channels=image_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        fw_layers.append(nn.ReLU())\n",
    "        fw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        \n",
    "        self.conv_forward = nn.Sequential(*fw_layers)\n",
    "\n",
    "        bw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        bw_layers.append(nn.ReLU())\n",
    "        bw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=image_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        \n",
    "        self.conv_backward = nn.Sequential(*bw_layers)\n",
    "\n",
    "    def forward(self, x, PhiTPhi, PhiTb):\n",
    "\n",
    "        x = x - self.lambda_step * torch.matmul(PhiTPhi,x)\n",
    "        x_input = x + self.lambda_step * PhiTb\n",
    "        x_input = x_input.view(-1, 3, 64, 64)\n",
    "\n",
    "        x_forward = self.conv_forward(x_input)\n",
    "\n",
    "        x_st = torch.matmul(torch.sign(x_forward), F.relu(torch.abs(x_forward) - self.soft_thr))\n",
    "       \n",
    "        x_backward = self.conv_backward(x_st)\n",
    "        \n",
    "        x_pred = x_backward\n",
    "        \n",
    "        x_est = self.conv_backward(x_forward)\n",
    "\n",
    "        symloss = x_est - x_input\n",
    "\n",
    "        return [x_pred, symloss, x_st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30825598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ISTA-Net\n",
    "class ISTANet(torch.nn.Module):\n",
    "    def __init__(self, LayerNo):\n",
    "        super(ISTANet, self).__init__()\n",
    "        layer = []\n",
    "        self.LayerNo = LayerNo\n",
    "\n",
    "        for i in range(LayerNo):\n",
    "            layer.append(BasicBlock())\n",
    "\n",
    "        self.fcs = nn.ModuleList(layer)\n",
    "\n",
    "    def forward(self, Phix, Phi, Qinit):\n",
    "\n",
    "        PhiTPhi = torch.matmul(Phi.T, Phi)\n",
    "        PhiTb = torch.matmul(Phi.T, Phix)\n",
    "\n",
    "        x = torch.matmul(Qinit,Phix)\n",
    "\n",
    "        layers_sym = []   # for computing symmetric loss\n",
    "        layer_st = []\n",
    "\n",
    "        for i in range(self.LayerNo):\n",
    "            [x, layer_sym, layer_st] = self.fcs[i](x, PhiTPhi, PhiTb)\n",
    "            layers_sym.append(layer_sym)\n",
    "\n",
    "        x_final = x\n",
    "\n",
    "        return [x_final, layers_sym, layer_st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CS_istanet(img_batch,test_batch,layer_num,learning_rate,epoch,gamma,cs_ratio):\n",
    "\n",
    "    cs = int(64*cs_ratio)\n",
    "    model = ISTANet(layer_num)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    to_img = transforms.ToPILImage()\n",
    "\n",
    "    np.random.seed(123)\n",
    "    Phi = np.random.normal(size=(64,64))\n",
    "    Phi = orth(Phi.T).T\n",
    "    Phi = torch.tensor(Phi)[:cs].float().to(device)\n",
    "\n",
    "    loss_list = []\n",
    "    loss_all = torch.tensor(0).to(device)\n",
    "    stop = False\n",
    "    # Training loop  \n",
    "    for epoch_i in range(1, epoch+1):\n",
    "        for i,img1 in enumerate(img_batch):\n",
    "            loss_old = loss_all.clone()\n",
    "            target = torch.tensor(img1)\n",
    "            target = target.float().to(device)\n",
    "            #Phix = torch.matmul(Phi,target).float()\n",
    "            Phix = torch.matmul(Phi,target).float()\n",
    "            Phix = Phix + 0.01*torch.randn_like(Phix)\n",
    "            Qinit = torch.linalg.inv(Phix.matmul(Phix.permute(0,1,3,2)))\n",
    "            Qinit = (target.matmul(Phix.permute(0,1,3,2))).matmul(Qinit)\n",
    "            Qinit = Qinit.to(device)\n",
    "          \n",
    "            [x_output, loss_layers_sym, loss_st] = model(Phix, Phi, Qinit)\n",
    "\n",
    "            # Compute and print loss\n",
    "            loss_discrepancy = torch.mean(torch.pow(x_output - target, 2))\n",
    "            loss_constraint = torch.mean(torch.pow(loss_layers_sym[0],2))\n",
    "            for k in range(layer_num-1):\n",
    "                loss_constraint += torch.mean(torch.pow(loss_layers_sym[k+1],2))\n",
    "\n",
    "            sparsity_constraint = 0\n",
    "            for k,_ in enumerate(loss_st, 0):\n",
    "                sparsity_constraint += torch.mean(torch.abs(loss_st[k]))\n",
    "\n",
    "            #loss_all = loss_discrepancy\n",
    "            loss_all = loss_discrepancy + 0.01 * loss_constraint + 0.001 * sparsity_constraint\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss_all.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #if i == 0:\n",
    "                #f epoch_i in range(1,epoch+1,5):\n",
    "                    #for i,(j,k,_) in enumerate(x_output):\n",
    "            #print('Epoch: ',epoch_i,'MSE: ',loss_all.cpu().data.numpy())\n",
    "            #    plt.figure(figsize=(20,120))\n",
    "            #    ax = plt.subplot(1, 6, 1) \n",
    "            #    img = to_img(target[0])\n",
    "            #    plt.imshow(img)\n",
    "              #    ax.set_xticks([])  \n",
    "              #    ax.set_yticks([])\n",
    "              #    ax = plt.subplot(1, 6, 2)\n",
    "              #    img = to_img(Phix[0])\n",
    "              #    plt.imshow(img)\n",
    "              #    ax.set_xticks([])  \n",
    "              #    ax.set_yticks([])\n",
    "              #    ax = plt.subplot(1, 6, 3)\n",
    "              #    img = to_img(x_output[0])\n",
    "              #    plt.imshow(img)\n",
    "              #    ax.set_xticks([])  \n",
    "              #    ax.set_yticks([])\n",
    "              #    ax = plt.subplot(1, 6, 4) \n",
    "              #    img = to_img(x_output[0][0])\n",
    "              #    plt.imshow(img)\n",
    "              #    ax.set_xticks([])  \n",
    "              #    ax.set_yticks([])\n",
    "              #    ax = plt.subplot(1, 6, 5) \n",
    "              #    img = to_img(x_output[0][1])\n",
    "              #    plt.imshow(img)\n",
    "              #    ax.set_xticks([])  \n",
    "              #    ax.set_yticks([])\n",
    "              #    ax = plt.subplot(1, 6, 6) \n",
    "              #    img = to_img(x_output[0][2])\n",
    "              #    plt.imshow(img)\n",
    "              #    ax.set_xticks([])  \n",
    "              #    ax.set_yticks([]) \n",
    "              #    plt.show()\n",
    " \n",
    "            loss_list.append(loss_all.cpu().data.numpy())\n",
    "            res_loss = np.abs(loss_old.cpu().data.numpy() - loss_all.cpu().data.numpy())\n",
    "            if res_loss < 1e-8:\n",
    "                stop = True\n",
    "                break\n",
    "\n",
    "        if stop:\n",
    "            break\n",
    "    path = 'ISTA-Net'+ str(layer_num) + str(cs_ratio) + '.pkl'\n",
    "    torch.save(model,path)\n",
    "\n",
    "    # if epoch_i == epoch:\n",
    "    print('layer_num: ',layer_num,' lr: ',learning_rate)\n",
    "    plt.plot(range(len(loss_list)),loss_list, '-')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.show()\n",
    "\n",
    "    print(' iter:',len(loss_list), ' MSE:',loss_list[-1])\n",
    "  \n",
    "    np.random.seed(123)\n",
    "    Phi = np.random.normal(size=(64,64))\n",
    "    Phi = orth(Phi.T).T\n",
    "    Phi = torch.tensor(Phi[:cs]).float().to(device)\n",
    "    mse = []\n",
    "    for i,img1 in enumerate(img_batch):\n",
    "        target = torch.tensor(img1)\n",
    "        target = target.float().to(device)\n",
    "        Phix = torch.matmul(Phi,target).float() + 0.01*torch.randn_like(Phi)\n",
    "        Qinit = torch.linalg.inv(Phix.matmul(Phix.permute(0,1,3,2)))\n",
    "        Qinit = (target.matmul(Phix.permute(0,1,3,2))).matmul(Qinit)\n",
    "        Qinit = Qinit.to(device)\n",
    "        [x_output, loss_layers_sym, loss_st] = model(Phix, Phi, Qinit)\n",
    "        mse.append(torch.mean(torch.pow(x_output - target, 2)).cpu().data.numpy())\n",
    "        for i in range(1,2):\n",
    "            plt.figure(figsize=(20,120))\n",
    "            ax = plt.subplot(1, 6, 1) \n",
    "            img = to_img(target[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 2)\n",
    "            img = to_img(Phix[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([]) \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 3)\n",
    "            img = to_img(x_output[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([]) \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 4) \n",
    "            img = to_img(x_output[i][0])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([]) \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 5) \n",
    "            img = to_img(x_output[i][1])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([]) \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 6) \n",
    "            img = to_img(x_output[i][2])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([]) \n",
    "            ax.set_yticks([]) \n",
    "            plt.show()\n",
    "    MSE = np.mean(mse)\n",
    "    PRSN = round(20 * math.log10(1 / math.sqrt(MSE)),3)\n",
    "    print('training MSE: ',MSE)\n",
    "    print('training PRSN: ',PRSN)\n",
    "    print('========'*5)\n",
    "    np.random.seed(123)\n",
    "    Phi = np.random.normal(size=(64,64))\n",
    "    Phi = orth(Phi.T).T\n",
    "    Phi = torch.tensor(Phi[:cs]).float().to(device)\n",
    "    mse = []\n",
    "    for i,img1 in enumerate(test_batch):\n",
    "        target = torch.tensor(img1)\n",
    "        target = target.float().to(device)\n",
    "        Phix = torch.matmul(Phi,target).float() + 0.01*torch.randn_like(Phi)\n",
    "        Qinit = torch.linalg.inv(Phix.matmul(Phix.permute(0,1,3,2)))\n",
    "        Qinit = (target.matmul(Phix.permute(0,1,3,2))).matmul(Qinit)\n",
    "        Qinit = Qinit.to(device)\n",
    "        [x_output, loss_layers_sym, loss_st] = model(Phix, Phi, Qinit)\n",
    "        mse.append(torch.mean(torch.pow(x_output - target, 2)).cpu().data.numpy())\n",
    "        for i in range(1,2):\n",
    "            plt.figure(figsize=(20,120))\n",
    "            ax = plt.subplot(1, 6, 1) \n",
    "            img = to_img(target[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 2)\n",
    "            img = to_img(Phix[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([]) \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 3)\n",
    "            img = to_img(x_output[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([]) \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 4) \n",
    "            img = to_img(x_output[i][0])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 5) \n",
    "            img = to_img(x_output[i][1])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 6) \n",
    "            img = to_img(x_output[i][2])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([]) \n",
    "            plt.show()\n",
    "    MSE = np.mean(mse)\n",
    "    PRSN = round(20 * math.log10(1 / math.sqrt(MSE)),3)\n",
    "    print('training MSE: ',MSE)\n",
    "    print('training PRSN: ',PRSN)\n",
    "    print('======'*5)\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee6a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "istanet = {}\n",
    "for i in [0.25,0.5,0.75,1]:\n",
    "    for j in [3]:\n",
    "        istanet[str(i)] = CS_istanet(train_gt_gau_batch,test_gt_gau_batch,layer_num=j,learning_rate=0.0001,epoch=200,gamma=0.01,cs_ratio=i)\n",
    "        print('CS ratio: ',i,'\\n Layer number: ',j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75157ad",
   "metadata": {},
   "source": [
    "# ISTA-Net+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7aea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ISTA-Net+ Block\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    def __init__(self,n_channels=128, image_channels=3,kernel_size=3):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        fw_layers = []\n",
    "        bw_layers = []\n",
    " \n",
    "        self.lambda_step = nn.Parameter(torch.Tensor([1]))\n",
    "        self.soft_thr = nn.Parameter(torch.Tensor([0.5]))\n",
    "\n",
    "        fw_layers.append(nn.Conv2d(in_channels=image_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        fw_layers.append(nn.ReLU())\n",
    "        fw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        \n",
    "        self.conv_forward = nn.Sequential(*fw_layers)\n",
    "\n",
    "        bw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        bw_layers.append(nn.ReLU())\n",
    "        bw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=image_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        \n",
    "        self.conv_backward = nn.Sequential(*bw_layers)\n",
    "\n",
    "    def forward(self, x, PhiTPhi, PhiTb):\n",
    "\n",
    "        x = x - self.lambda_step * torch.matmul(PhiTPhi,x)\n",
    "        x_input = x + self.lambda_step * PhiTb\n",
    "        x_input = x_input.view(-1, 3, 64, 64)\n",
    "\n",
    "        x_forward = self.conv_forward(x_input)\n",
    "\n",
    "        x_st = torch.matmul(torch.sign(x_forward), F.relu(torch.abs(x_forward) - self.soft_thr))\n",
    "       \n",
    "        x_backward = self.conv_backward(x_st)\n",
    "        \n",
    "        #x_pred = x_backward\n",
    "        x_pred = F.relu(x_input + x_backward)\n",
    "        \n",
    "        x_est = self.conv_backward(x_forward)\n",
    "\n",
    "        symloss = x_est - x_input\n",
    "\n",
    "        return [x_pred, symloss, x_st]\n",
    "\n",
    "# Define ISTA-Net+\n",
    "class ISTANetp(torch.nn.Module):\n",
    "    def __init__(self, LayerNo):\n",
    "        super(ISTANetp, self).__init__()\n",
    "        layer = []\n",
    "        self.LayerNo = LayerNo\n",
    "\n",
    "        for i in range(LayerNo):\n",
    "            layer.append(BasicBlock())\n",
    "\n",
    "        self.fcs = nn.ModuleList(layer)\n",
    "\n",
    "    def forward(self, Phix, Phi, Qinit):\n",
    "\n",
    "        PhiTPhi = torch.matmul(Phi.T, Phi)\n",
    "        PhiTb = torch.matmul(Phi.T, Phix)\n",
    "\n",
    "        x = torch.matmul(Qinit,Phix)\n",
    "\n",
    "        layers_sym = []   # for computing symmetric loss\n",
    "        layer_st = []\n",
    "\n",
    "        for i in range(self.LayerNo):\n",
    "            [x, layer_sym, layer_st] = self.fcs[i](x, PhiTPhi, PhiTb)\n",
    "            layers_sym.append(layer_sym)\n",
    "\n",
    "        x_final = x\n",
    "\n",
    "        return [x_final, layers_sym, layer_st]\n",
    "\n",
    "def CS_istanetp(img_batch,test_batch,layer_num,learning_rate,epoch,gamma,cs_ratio):\n",
    "  \n",
    "    cs = int(64*cs_ratio)\n",
    "    model = ISTANetp(layer_num)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    to_img = transforms.ToPILImage()\n",
    "\n",
    "    np.random.seed(123)\n",
    "    Phi = np.random.normal(size=(64,64))\n",
    "    Phi = orth(Phi.T).T\n",
    "    Phi = torch.tensor(Phi[:cs]).float().to(device)\n",
    "\n",
    "    loss_list = []\n",
    "    loss_all = torch.tensor(0).to(device)\n",
    "    stop = False\n",
    "    # Training loop  \n",
    "    for epoch_i in range(1, epoch+1):\n",
    "        for i,img1 in enumerate(img_batch):\n",
    "            loss_old = loss_all.clone()\n",
    "            target = torch.tensor(img1)\n",
    "            target = target.float().to(device)\n",
    "            #Phix = torch.matmul(Phi,target).float()\n",
    "            Phix = torch.matmul(Phi,target).float() + 0.01*torch.randn_like(Phi)\n",
    "            Qinit = torch.linalg.inv(Phix.matmul(Phix.permute(0,1,3,2)))\n",
    "            Qinit = (target.matmul(Phix.permute(0,1,3,2))).matmul(Qinit)\n",
    "            Qinit = Qinit.to(device)\n",
    "          \n",
    "            [x_output, loss_layers_sym, loss_st] = model(Phix, Phi, Qinit)\n",
    "\n",
    "            # Compute and print loss\n",
    "            loss_discrepancy = torch.mean(torch.pow(x_output - target, 2))\n",
    "            loss_constraint = torch.mean(torch.pow(loss_layers_sym[0],2))\n",
    "            for k in range(layer_num-1):\n",
    "                loss_constraint += torch.mean(torch.pow(loss_layers_sym[k+1],2))\n",
    "\n",
    "            sparsity_constraint = 0\n",
    "            for k,_ in enumerate(loss_st, 0):\n",
    "                sparsity_constraint += torch.mean(torch.abs(loss_st[k]))\n",
    "\n",
    "            #loss_all = loss_discrepancy\n",
    "            loss_all = loss_discrepancy + 0.01 * loss_constraint + 0.001 * sparsity_constraint\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss_all.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append(loss_all.cpu().data.numpy())\n",
    "            res_loss = np.abs(loss_old.cpu().data.numpy() - loss_all.cpu().data.numpy())\n",
    "            if res_loss < 1e-8:\n",
    "                stop = True\n",
    "                break\n",
    "        if stop:\n",
    "            break\n",
    "    path = 'ISTA-Netp'+ str(layer_num) + str(cs_ratio) + '.pkl'\n",
    "    torch.save(model,path) \n",
    "\n",
    "    # if epoch_i == epoch:\n",
    "    print('layer_num: ',layer_num,' lr: ',learning_rate)\n",
    "    plt.plot(range(len(loss_list)),loss_list, '-')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.show()\n",
    "    print(' iter:',len(loss_list), ' MSE:',loss_list[-1])\n",
    "\n",
    "    np.random.seed(123)\n",
    "    Phi = np.random.normal(size=(64,64))\n",
    "    Phi = orth(Phi.T).T\n",
    "    Phi = torch.tensor(Phi[:cs]).float().to(device)\n",
    "    mse = []\n",
    "    for i,img1 in enumerate(img_batch):\n",
    "        target = torch.tensor(img1)\n",
    "        target = target.float().to(device)\n",
    "        Phix = torch.matmul(Phi,target).float() + 0.01*torch.randn_like(Phi)\n",
    "        Qinit = torch.linalg.inv(Phix.matmul(Phix.permute(0,1,3,2)))\n",
    "        Qinit = (target.matmul(Phix.permute(0,1,3,2))).matmul(Qinit)\n",
    "        Qinit = Qinit.to(device)\n",
    "        [x_output, loss_layers_sym, loss_st] = model(Phix, Phi, Qinit)\n",
    "        mse.append(torch.mean(torch.pow(x_output - target, 2)).cpu().data.numpy())\n",
    "        for i in range(1,2):\n",
    "            plt.figure(figsize=(20,120))\n",
    "            ax = plt.subplot(1, 6, 1) \n",
    "            img = to_img(target[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 2)\n",
    "            img = to_img(Phix[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 3)\n",
    "            img = to_img(x_output[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 4) \n",
    "            img = to_img(x_output[i][0])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 5) \n",
    "            img = to_img(x_output[i][1])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 6) \n",
    "            img = to_img(x_output[i][2])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([]) \n",
    "            plt.show()\n",
    "    MSE = np.mean(mse)\n",
    "    PRSN = round(20 * math.log10(1 / math.sqrt(MSE)),3)\n",
    "    print('training MSE: ',MSE)\n",
    "    print('training PRSN: ',PRSN)\n",
    "    print('========'*5)\n",
    "    np.random.seed(123)\n",
    "    Phi = np.random.normal(size=(64,64))\n",
    "    Phi = orth(Phi.T).T\n",
    "    Phi = torch.tensor(Phi[:cs]).float().to(device)\n",
    "    mse = []\n",
    "    for i,img1 in enumerate(test_batch):\n",
    "        target = torch.tensor(img1)\n",
    "        target = target.float().to(device)\n",
    "        Phix = torch.matmul(Phi,target).float() + 0.01*torch.randn_like(Phi)\n",
    "        Qinit = torch.linalg.inv(Phix.matmul(Phix.permute(0,1,3,2)))\n",
    "        Qinit = (target.matmul(Phix.permute(0,1,3,2))).matmul(Qinit)\n",
    "        Qinit = Qinit.to(device)\n",
    "        [x_output, loss_layers_sym, loss_st] = model(Phix, Phi, Qinit)\n",
    "        mse.append(torch.mean(torch.pow(x_output - target, 2)).cpu().data.numpy())\n",
    "        for i in range(1,2):\n",
    "            plt.figure(figsize=(20,120))\n",
    "            ax = plt.subplot(1, 6, 1) \n",
    "            img = to_img(target[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 2)\n",
    "            img = to_img(Phix[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 3)\n",
    "            img = to_img(x_output[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 4) \n",
    "            img = to_img(x_output[i][0])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([]) \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 5) \n",
    "            img = to_img(x_output[i][1])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([]) \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 6) \n",
    "            img = to_img(x_output[i][2])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([]) \n",
    "            plt.show()\n",
    "    MSE = np.mean(mse)\n",
    "    PRSN = round(20 * math.log10(1 / math.sqrt(MSE)),3)\n",
    "    print('training MSE: ',MSE)\n",
    "    print('training PRSN: ',PRSN)\n",
    "    print('======'*5)\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd172e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "istanetp = {}\n",
    "for i in [0.25,0.5,0.75,1]:\n",
    "    for j in [3]:\n",
    "        istanetp[str(i)] = CS_istanetp(train_gt_gau_batch,test_gt_gau_batch,layer_num=j,learning_rate=0.0001,epoch=200,gamma=0.01,cs_ratio=i)\n",
    "        print('CS ratio: ',i,'\\n layer number: ',j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8dd9b7",
   "metadata": {},
   "source": [
    "# FISTA-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd79ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define basic block of FISTA-Net\n",
    "class  BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels=128, image_channels=3, kernel_size=3,padding=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.Sp = nn.Softplus()\n",
    "\n",
    "        fw_layers = []\n",
    "        bw_layers = []\n",
    " \n",
    "        self.lambda_step = nn.Parameter(torch.Tensor([1]))\n",
    "        self.soft_thr = nn.Parameter(torch.Tensor([0.5]))\n",
    "\n",
    "        fw_layers.append(nn.Conv2d(in_channels=image_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        fw_layers.append(nn.ReLU())\n",
    "        fw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        fw_layers.append(nn.ReLU())\n",
    "        fw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        fw_layers.append(nn.ReLU())\n",
    "        fw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        \n",
    "        self.conv_forward = nn.Sequential(*fw_layers)\n",
    "        \n",
    "        bw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        bw_layers.append(nn.ReLU())        \n",
    "        bw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        bw_layers.append(nn.ReLU())\n",
    "        bw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        bw_layers.append(nn.ReLU())\n",
    "        bw_layers.append(nn.Conv2d(in_channels=n_channels,out_channels=image_channels,kernel_size=kernel_size,padding=padding,bias=True))\n",
    "        \n",
    "        self.conv_backward = nn.Sequential(*bw_layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, PhiTPhi, PhiTb, lambda_step, soft_thr):\n",
    "        \n",
    "        # naive gradient descent update\n",
    "        x = x - self.Sp(lambda_step)  * PhiTPhi.matmul(x) + self.Sp(lambda_step) * PhiTb\n",
    "\n",
    "        x_input = x.view(-1,3,64,64)\n",
    "\n",
    "        x_forward = self.conv_forward(x_input)\n",
    "\n",
    "        # soft-thresholding block\n",
    "        x_st = torch.mul(torch.sign(x_forward), F.relu(torch.abs(x_forward) - self.Sp(soft_thr)))\n",
    "\n",
    "        x_backward = self.conv_backward(x_st)\n",
    "\n",
    "        # prediction output (skip connection); non-negative output\n",
    "        x_pred = F.relu(x_input + x_backward)\n",
    "\n",
    "        # compute symmetry loss\n",
    "        x_input = self.conv_backward(x_forward)\n",
    "\n",
    "        symloss = x_input - x_input\n",
    "\n",
    "        return [x_pred, symloss, x_st]\n",
    "\n",
    "class FISTANet(nn.Module):\n",
    "    def __init__(self, LayerNo):\n",
    "        super(FISTANet, self).__init__()\n",
    "        self.LayerNo = LayerNo\n",
    "        layer = []\n",
    "\n",
    "        for i in range(LayerNo):\n",
    "            layer.append(BasicBlock())\n",
    "\n",
    "        self.fcs = nn.ModuleList(layer)\n",
    "        \n",
    "        # thresholding value\n",
    "        self.w_theta = nn.Parameter(torch.Tensor([-0.5]))\n",
    "        self.b_theta = nn.Parameter(torch.Tensor([-2]))\n",
    "        # gradient step\n",
    "        self.w_mu = nn.Parameter(torch.Tensor([-0.2]))\n",
    "        self.b_mu = nn.Parameter(torch.Tensor([0.1]))\n",
    "        # two-step update weight\n",
    "        self.w_rho = nn.Parameter(torch.Tensor([0.5]))\n",
    "        self.b_rho = nn.Parameter(torch.Tensor([0]))\n",
    "\n",
    "        self.Sp = nn.Softplus()\n",
    "\n",
    "    def forward(self, Phix, Phi, Qinit):\n",
    "\n",
    "        PhiTPhi = torch.matmul(Phi.T, Phi)\n",
    "        PhiTb = torch.matmul(Phi.T, Phix)\n",
    "\n",
    "        x = torch.matmul(Qinit,Phix)\n",
    "\n",
    "        # initialize the result\n",
    "        xold = x\n",
    "        y = xold \n",
    "        layers_sym = []     # for computing symmetric loss\n",
    "        layers_st = []      # for computing sparsity constraint\n",
    "        xnews = []       # iteration result\n",
    "        xnews.append(xold)\n",
    "\n",
    "        for i in range(self.LayerNo):\n",
    "            theta_ = self.w_theta * i + self.b_theta\n",
    "            mu_ = self.w_mu * i + self.b_mu\n",
    "            [xnew, layer_sym, layer_st] = self.fcs[i](y, PhiTPhi, PhiTb, mu_, theta_)\n",
    "            rho_ = 1 - self.Sp(self.w_rho + self.b_rho) / self.Sp(self.w_rho * i + self.b_rho)\n",
    "            y = xnew + rho_ * (xnew - xold) # two-step update\n",
    "            xold = xnew\n",
    "            xnews.append(xnew)   # iteration result\n",
    "            layers_st.append(layer_st)\n",
    "            layers_sym.append(layer_sym)\n",
    "\n",
    "        return [xnew, layers_sym, layers_st]\n",
    "\n",
    "def CS_fistanet(img_batch,test_batch,layer_num,learning_rate,epoch,gamma,cs_ratio):\n",
    "\n",
    "    model = FISTANet(layer_num)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    to_img = transforms.ToPILImage()\n",
    "    cs = int(64*cs_ratio)\n",
    "\n",
    "    np.random.seed(123)\n",
    "    Phi = np.random.normal(size=(64,64))\n",
    "    Phi = torch.tensor(orth(Phi.T).T)\n",
    "    Phi = Phi[:cs].float().to(device)\n",
    "\n",
    "    loss_list = []\n",
    "    loss_all = torch.tensor(0).to(device)\n",
    "    stop = False\n",
    "    # Training loop  \n",
    "    for epoch_i in range(1, epoch+1):\n",
    "        for i,img1 in enumerate(img_batch):\n",
    "            loss_old = loss_all\n",
    "            target = torch.tensor(img1)\n",
    "            target = target.float().to(device)\n",
    "            Phix = torch.matmul(Phi,target).float() + 0.01 * torch.randn_like(Phi)\n",
    "            Qinit = torch.linalg.inv(Phix.matmul(Phix.permute(0,1,3,2)))\n",
    "            Qinit = (target.matmul(Phix.permute(0,1,3,2))).matmul(Qinit)\n",
    "            Qinit = Qinit.to(device)\n",
    "            [x_output, loss_layers_sym, loss_st] = model(Phix, Phi, Qinit)          \n",
    "                    \n",
    "            # Compute loss, data consistency and regularizer constraints\n",
    "            loss_discrepancy = torch.mean(torch.pow(x_output - target, 2)) # + l1_loss(x_output, target, 0.1)\n",
    "\n",
    "            loss_constraint = torch.mean(torch.pow(loss_layers_sym[0],2))\n",
    "            for k in range(layer_num-1):\n",
    "                loss_constraint += torch.mean(torch.pow(loss_layers_sym[k+1],2))\n",
    "                    \n",
    "            sparsity_constraint = 0\n",
    "            for k, _ in enumerate(loss_st, 0):\n",
    "                sparsity_constraint += torch.mean(torch.abs(loss_st[k]))\n",
    "                    \n",
    "            #loss_all = loss_discrepancy + gamma * loss_constraint\n",
    "            loss_all = loss_discrepancy +  0.01 * loss_constraint + 0.001 * sparsity_constraint\n",
    "       \n",
    "            #model.zero_grad()\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss_all.backward()\n",
    "            optimizer.step()                \n",
    "         \n",
    "            loss_list.append(loss_all.cpu().data.numpy())\n",
    "            res_loss = np.abs(loss_old.cpu().data.numpy()-loss_all.cpu().data.numpy())\n",
    "            if res_loss < 1e-8:\n",
    "                stop = True\n",
    "                break\n",
    "\n",
    "        if stop:\n",
    "            break\n",
    "    path = 'FISTA-Net_parameter'+ str(layer_num) + str(cs_ratio) + '.pkl'\n",
    "    torch.save(model.state_dict(),path) \n",
    "     \n",
    "    print('layer_num: ',layer_num,' lr: ',learning_rate)\n",
    "    plt.plot(range(len(loss_list)),loss_list, '-')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.show()\n",
    "    print(' iter:',len(loss_list), ' MSE:',loss_list[-1])\n",
    "\n",
    "    np.random.seed(123)\n",
    "    Phi = np.random.normal(size=(64,64))\n",
    "    Phi = orth(Phi.T).T\n",
    "    Phi = torch.tensor(Phi[:cs]).float().to(device)\n",
    "    mse = []\n",
    "    for i,img1 in enumerate(img_batch):\n",
    "        target = torch.tensor(img1)\n",
    "        target = target.float().to(device)\n",
    "        Phix = torch.matmul(Phi,target).float() + 0.01*torch.randn_like(Phi)\n",
    "        Qinit = torch.linalg.inv(Phix.matmul(Phix.permute(0,1,3,2)))\n",
    "        Qinit = (target.matmul(Phix.permute(0,1,3,2))).matmul(Qinit)\n",
    "        Qinit = Qinit.to(device)\n",
    "        [x_output, loss_layers_sym, loss_st] = model(Phix, Phi, Qinit)\n",
    "        mse.append(torch.mean(torch.pow(x_output - target, 2)).cpu().data.numpy())\n",
    "        for i in range(1,2):\n",
    "            plt.figure(figsize=(20,120))\n",
    "            ax = plt.subplot(1, 6, 1) \n",
    "            img = to_img(target[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 2)\n",
    "            img = to_img(Phix[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 3)\n",
    "            img = to_img(x_output[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 4) \n",
    "            img = to_img(x_output[i][0])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 5) \n",
    "            img = to_img(x_output[i][1])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 6) \n",
    "            img = to_img(x_output[i][2])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([]) \n",
    "            plt.show()\n",
    "    MSE = np.mean(mse)\n",
    "    PRSN = round(20 * math.log10(1 / math.sqrt(MSE)),3)\n",
    "    print('training MSE: ',MSE)\n",
    "    print('training PRSN: ',PRSN)\n",
    "    print('========'*5)\n",
    "    np.random.seed(123)\n",
    "    Phi = np.random.normal(size=(64,64))\n",
    "    Phi = orth(Phi.T).T\n",
    "    Phi = torch.tensor(Phi[:cs]).float().to(device)\n",
    "    mse = []\n",
    "    for i,img1 in enumerate(test_batch):\n",
    "        target = torch.tensor(img1)\n",
    "        target = target.float().to(device)\n",
    "        Phix = torch.matmul(Phi,target).float() + 0.01*torch.randn_like(Phi)\n",
    "        Qinit = torch.linalg.inv(Phix.matmul(Phix.permute(0,1,3,2)))\n",
    "        Qinit = (target.matmul(Phix.permute(0,1,3,2))).matmul(Qinit)\n",
    "        Qinit = Qinit.to(device)\n",
    "        [x_output, loss_layers_sym, loss_st] = model(Phix, Phi, Qinit)\n",
    "        mse.append(torch.mean(torch.pow(x_output - target, 2)).cpu().data.numpy())\n",
    "        for i in range(1,2):\n",
    "            plt.figure(figsize=(20,120))\n",
    "            ax = plt.subplot(1, 6, 1) \n",
    "            img = to_img(target[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 2)\n",
    "            img = to_img(Phix[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 3)\n",
    "            img = to_img(x_output[i])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 4) \n",
    "            img = to_img(x_output[i][0])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 5) \n",
    "            img = to_img(x_output[i][1])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([]) \n",
    "            ax.set_yticks([])\n",
    "            ax = plt.subplot(1, 6, 6) \n",
    "            img = to_img(x_output[i][2])\n",
    "            plt.imshow(img)\n",
    "            ax.set_xticks([])  \n",
    "            ax.set_yticks([]) \n",
    "            plt.show()    \n",
    "\n",
    "    MSE = np.mean(mse)\n",
    "    PRSN = round(20 * math.log10(1 / math.sqrt(MSE)),3)\n",
    "    print('training MSE: ',MSE)\n",
    "    print('training PRSN: ',PRSN)\n",
    "    print('======'*5)\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d304ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addictive Gaussian\n",
    "train_gt_gau,train_noise_gau = dataset(train_set,size=1024,seed=123,noise_type='gaussian',noise_num=500,multi=True)\n",
    "test_gt_gau,test_noise_gau = dataset(test_set,size=256,seed=123,noise_type='gaussian',noise_num=500,multi=True)\n",
    "batch_size = 64\n",
    "train_noise_gau_batch = DataLoader(train_noise_gau, batch_size=batch_size, shuffle=False)\n",
    "train_gt_gau_batch = DataLoader(train_gt_gau, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_noise_gau_batch = DataLoader(test_noise_gau, batch_size=batch_size, shuffle=False,num_workers=0)\n",
    "test_gt_gau_batch = DataLoader(test_gt_gau, batch_size=batch_size, shuffle=False,num_workers=0)\n",
    "\n",
    "%%time\n",
    "fista = {}\n",
    "for i in [0.25,0.5,0.75,1]:\n",
    "    for j in [3]:\n",
    "        fista[str(i)]=CS_fistanet(train_gt_gau_batch,test_gt_gau_batch,layer_num=j,learning_rate=0.0001,epoch=200,gamma=0.01,cs_ratio=i)\n",
    "        print('CS ratio: ',i,'\\n layer number: ',j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
